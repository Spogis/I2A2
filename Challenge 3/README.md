# Apresentação Grupo 3 - I2A2

### Integrantes:
- Ana Oliveira 
- André Souza 
- Bruno de Sousa Donato 
- Lucas dos Santos Garcia 
- Nicolas Spogis 
- Roger Trezza 
- Thiago Martins 
- Thomaz Barros 
- Vinicius Vizenzo

### Temas:
- Ensemble Methods
- Boosting
- Bagging
- Random Forest
- Extra Trees

### Links
- [Relatório Completo](https://github.com/Spogis/I2A2/blob/master/Challenge%203/Relat%C3%B3rio%20Grupo%203.pdf)
- [PPT](https://github.com/Spogis/I2A2/blob/master/Challenge%203/PPT%20-%20Grupo%203.pptx)
- [PPT em formato PDF](https://github.com/Spogis/I2A2/blob/master/Challenge%203/PPT%20-%20Grupo%203.pdf)
#### Exemplos de casos
- [Exemplos Random Forest](https://github.com/Spogis/I2A2/blob/master/Challenge%203/Jupyter%20Notebooks/00%20-%20Random%20Forest%20Regression.ipynb)
- [Extra Trees](https://github.com/Spogis/I2A2/blob/master/Challenge%203/Jupyter%20Notebooks/01%20-%20Extremely%20Randomized%20Trees%20Regression.ipynb)
- [Dataset](https://github.com/Spogis/I2A2/blob/master/Challenge%203/Jupyter%20Notebooks/TabelaTACO.xlsx)


### Aulas Canal Yotube (Nicolas Spogis)
#### Aula Random Forest
[![Aula Random Forest](https://img.youtube.com/vi/v236RYHjt08/0.jpg)](https://youtu.be/v236RYHjt08)
#### Aula Extra Trees
[![Aula Extra Trees](https://img.youtube.com/vi/0pYxoQAdFe4/0.jpg)](https://youtu.be/0pYxoQAdFe4)



### Objetivo

A apresentação visa esclarecer o funcionamento e a importância dessas técnicas, demonstrando sua aplicação prática e os ganhos obtidos em diferentes contextos de aprendizado de máquina.

### Introdução

Os métodos de aprendizado de máquina baseados em "ensembles" são amplamente utilizados devido à sua capacidade de melhorar significativamente a precisão das previsões. Em essência, eles combinam múltiplos modelos fracos para formar um modelo forte, fornecendo previsões mais robustas e confiáveis.

#### Importância dos Temas:

**Ensemble Methods:**  
Os métodos de ensemble são técnicas que combinam múltiplos modelos para obter melhores previsões. Eles são especialmente eficazes quando diferentes modelos capturam diferentes padrões nos dados.

**Boosting:**  
Boosting é uma técnica que ajusta modelos sequencialmente, dando mais peso aos exemplos mal classificados em iterações anteriores. Métodos populares como AdaBoost e Gradient Boosting têm sido essenciais para melhorar a precisão em competições de aprendizado de máquina.

**Bagging:**  
Bagging, ou Bootstrap Aggregating, é uma técnica que cria subconjuntos de dados aleatoriamente para treinar múltiplos modelos. A média ou votação desses modelos reduz a variância e melhora a estabilidade das previsões. 

**Random Forest:**  
Random Forest é uma extensão do bagging, utilizando múltiplas árvores de decisão treinadas em diferentes subconjuntos dos dados e com variáveis selecionadas aleatoriamente. Isso melhora a precisão e reduz o overfitting.

**Extra Trees:**  
Extra Trees (Extremely Randomized Trees) é uma variação do Random Forest que reduz ainda mais a variância ao introduzir maior aleatoriedade na escolha dos pontos de divisão das árvores.

